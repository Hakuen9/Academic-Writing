\chapter{Problem Characterization}
Microsoft revealed the creation of a new chatbot called ‘Tay’, on March 23 2016. Tay was named after the acronym:
“Thinking about you”. The chatbot interacted with twitter users and learned from the users’ input, it did so using
frequency analysis. Frequency analysis consists of evaluating phrases and words, more frequently appearing phrases
would have a heavier weight. And thus would be used more by the bot when engaging in conversations.

After 24 hours, however, the bot was taken offline for expressing anti-Semitic, racist and discriminating thoughts.
For example, when asked if Tay supported genocide, Tay responded with “i do indeed”.
It was clear that the behaviour was inappropriate so it had to be taken down, as a lot more people could have been
offended/ hurt by those tweets. Trolls and racist twitter users had influenced/ taught Tay to use inappropriate language.
This is an example of the abuse of the frequency analysis algorithm.
