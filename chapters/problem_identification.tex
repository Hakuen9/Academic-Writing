\chapter{Problem identification}
The chatbot (Tay) in itself did not malfunction on a technical level. The software did not contain flaws that resulted in a technical failure
(e.g. a disability ability to post new tweets).
On the contrary, it was not Microsoft's intention to make the bot post tweets to could be categorized as offensive.
The problem therefore has a much larger ethical aspect which has to be considered.

A question that arises is, could Microsoft have prevented this behavior by, for example, applying a filter to Tay’s communication layer?
Although Microsoft says that they “implemented a lot of filtering and conducted extensive user studies with diverse user groups.” (Peter Lee, 2016)
it remains unknown to what extend these tests were executed. In addition, if the conducted user studies really where extensive the vulnerability could maybe have been discovered at an earlier stage.
The possibility arises that the developers knew about the vulnerability prior to the release of Tay.
